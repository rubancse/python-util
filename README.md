1) S3 creates an event when a new file is uploaded.
2)The S3 event triggers an AWS Lambda function. Â  
3)The Lambda function uses the Databricks Jobs API to start a Databricks job, passing the S3 file path as a parameter.
The Databricks job processes the file.

4)This is the most direct and versatile way to start Databricks jobs programmatically. You'll need to make HTTP POST requests to the API endpoint.
curl -n -X POST \
  -H "Authorization: Bearer <your_databricks_token>" \
  -H "Content-Type: application/json" \
  -d '{
    "job_id": 123,
    "notebook_params": {
      "input_date": "2025-03-26",
      "data_source": "s3://your-bucket/data/"
    },
    "run_name": "Programmatic Run - March 26"
  }' \
  "https://<your_databricks_workspace>/api/2.1/jobs/run-now"

5) Get information about a specific job run
curl -n -X GET \
  -H "Authorization: Bearer <your_databricks_token>" \
  "https://<your_databricks_workspace>/api/2.1/jobs/runs/get?run_id=456"
